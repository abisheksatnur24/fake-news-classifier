{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "259f5aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe205d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data from CSV files\n",
    "train = pd.read_csv(\"Dataset/train.csv\")\n",
    "test = pd.read_csv(\"Dataset/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f016cee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                                                    title  \\\n",
       "0      Clinton faces pressure to pick VP who is tough...   \n",
       "1      Ryan, Trump cite 'positive step' toward Republ...   \n",
       "2       WATCH: President Obama Dares Republicans To S...   \n",
       "3      Hariri warns Lebanon faces Arab sanctions risk...   \n",
       "4       A POEM: ‘Twas The Night Before CNN’s Christmas…’   \n",
       "...                                                  ...   \n",
       "39893  LOL! PHOTO Accompanying Google Search Of “Path...   \n",
       "39894   What Trump Wants To Do To Your Children Would...   \n",
       "39895  Gay activists march through Serb capital behin...   \n",
       "39896                    Boiler Room #103 – Smoking Gunz   \n",
       "39897  No 'fire and fury' as Trump team talks North K...   \n",
       "\n",
       "                                                    text       subject  \\\n",
       "0      WASHINGTON (Reuters) - Members of the Democrat...  politicsNews   \n",
       "1      WASHINGTON (Reuters) - Presumptive Republican ...  politicsNews   \n",
       "2      Conservatives talk the talk but can they walk ...          News   \n",
       "3      BEIRUT (Reuters) - Saad al-Hariri warned on Su...     worldnews   \n",
       "4      ACR s BOILER ROOM presents a Christmas poem Tw...   Middle-east   \n",
       "...                                                  ...           ...   \n",
       "39893  The truth hurts  The idea that there are peopl...      politics   \n",
       "39894  Donald Trump recently gave a speech to the Ame...          News   \n",
       "39895  BELGRADE (Reuters) - Serbian gay right activis...     worldnews   \n",
       "39896  Tune in to the Alternate Current Radio Network...   Middle-east   \n",
       "39897  WASHINGTON (Reuters) - President Donald Trump ...     worldnews   \n",
       "\n",
       "                      date label  \n",
       "0           July 21, 2016   real  \n",
       "1            May 12, 2016   real  \n",
       "2             July 9, 2016  fake  \n",
       "3       November 12, 2017   real  \n",
       "4        December 25, 2017  fake  \n",
       "...                    ...   ...  \n",
       "39893         Oct 31, 2016  fake  \n",
       "39894    September 2, 2016  fake  \n",
       "39895  September 17, 2017   real  \n",
       "39896       March 30, 2017  fake  \n",
       "39897   September 6, 2017   real  \n",
       "\n",
       "[39898 rows x 5 columns]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View snapshot of data\n",
    "train.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f55fd1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\caleb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\caleb\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "# Define NLTK preprocessing functions\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "    # Lemmatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "    # Join the tokens back into a string\n",
    "    preprocessed_text = \" \".join(lemmatized_tokens)\n",
    "    return preprocessed_text\n",
    "\n",
    "# Apply the preprocessing function to the DataFrame column\n",
    "train[\"preprocessed_text\"] = train[\"text\"].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db04ad40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(train[\"preprocessed_text\"].iloc[0])\n",
    "test[\"preprocessed_text\"] = test[\"text\"].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8594fdb4",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 32.3 GiB for an array with shape (39898, 108645) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m test_bag_of_words \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mtransform(test[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreprocessed_text\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Convert the bag of words matrix to a DataFrame\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m train_bag \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mbag_of_words\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, columns\u001b[38;5;241m=\u001b[39mvectorizer\u001b[38;5;241m.\u001b[39mget_feature_names())\n\u001b[0;32m     16\u001b[0m test_bag \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(test_bag_of_words\u001b[38;5;241m.\u001b[39mtoarray(), columns\u001b[38;5;241m=\u001b[39mvectorizer\u001b[38;5;241m.\u001b[39mget_feature_names())\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(test_bag)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_compressed.py:1051\u001b[0m, in \u001b[0;36m_cs_matrix.toarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m order \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1050\u001b[0m     order \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_swap(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcf\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m-> 1051\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_toarray_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1052\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mc_contiguous \u001b[38;5;129;01mor\u001b[39;00m out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mf_contiguous):\n\u001b[0;32m   1053\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutput array must be C or F contiguous\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_base.py:1298\u001b[0m, in \u001b[0;36mspmatrix._process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m   1297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 32.3 GiB for an array with shape (39898, 108645) and data type int64"
     ]
    }
   ],
   "source": [
    "#print(test[\"preprocessed_text\"].iloc[0])\n",
    "#print(test[\"text\"].iloc[0])\n",
    "\n",
    "# Create a CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer to the preprocessed text data\n",
    "vectorizer.fit(train[\"preprocessed_text\"])\n",
    "\n",
    "# Transform the preprocessed text data into a bag of words matrix\n",
    "bag_of_words = vectorizer.transform(train[\"preprocessed_text\"])\n",
    "test_bag_of_words = vectorizer.transform(test[\"preprocessed_text\"])\n",
    "\n",
    "# Convert the bag of words matrix to a DataFrame\n",
    "train_bag = pd.DataFrame(bag_of_words.toarray(), columns=vectorizer.get_feature_names())\n",
    "test_bag = pd.DataFrame(test_bag_of_words.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "print(test_bag)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
